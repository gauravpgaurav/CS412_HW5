{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 5 : Mini-Project\n",
    "**Gaurav Pant** \n",
    "\n",
    "UIC CS 412, Spring 2018\n",
    "\n",
    "This project uses th following python files -\n",
    "\n",
    "* ``dataApp.py`` : This loads the dataset, handles the conversion of categorical data to numbers, and slits data into different categories\n",
    "\n",
    "* ``dt.py`` : This runs the Decision Tree Classifier on the dataset\n",
    "\n",
    "* ``randomForest.py``: This runs the Random Forest Classifier on the dataset\n",
    "\n",
    "* ``svm.py`` : This runs the Support Vector Classifier on the dataset\n",
    "\n",
    "* ``adaBoost.py``: This runs the AdaBoost Classifier on the dataset\n",
    "\n",
    "* ``mlp.py`` : This runs the Multi-layer Perceptron Classifier on the dataset\n",
    "\n",
    "* ``votingClassifier.py`` : This runs the Voting Classifier on the dataset\n",
    "\n",
    "\n",
    "## 1. The Task & Dataset\n",
    "This project aims to predict a person’s “empathy” on a scale from 1 to 5 using any of the other attributes in the dataset. From this rating, student volunteers can be recruited to help Alzheimer’s patients at a non-profit organization.\n",
    "\n",
    "The dataset used in this project is the [Young People Survey](https://www.kaggle.com/miroslavsabo/young-people-survey/) dataset, the data consists of 1010 responses of people, regarding 150 categories like - Music preferences, Personality traits, view on life & opinions etc. \n",
    "\n",
    "![Feature](image.png)\n",
    "\n",
    "As noticeable in the above graph, there is trend visible between gender and empathy. i.e. Females tend to have a higher empathy score. Thus, our goal is to find similiar preditions using highly capable machine learning tools.\n",
    "\n",
    "## 2. Preprocessing\n",
    "For any classifier to predict data accurately, the data must be 'clean'. i.e. Inconsistent data must be handled beforehand.\n",
    "\n",
    "1. Out of the 150 categories of data 11 were categorical columns, they were needed to preprocessed to numerical values. \n",
    "\n",
    "e.g. \n",
    "* The ``Gender`` column had two possible categorical values -\n",
    "\n",
    "``[Male, Female]``\n",
    "\n",
    "* On preprocessing -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['Gender'] = pd.Categorical(df['Gender'])\n",
    "df['Gender'] = df['Gender'].cat.codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The categorical values were now numerical (this is necessary for classification later on) -\n",
    "\n",
    "``[0-Male, 1-Female]``\n",
    "\n",
    "2. Another issue in the data was handling missing data. Specific data values were empty across various categories; these values are necessary to be filled with an appropriate value to proceed with the classification process.\n",
    "\n",
    "e.g. \n",
    "* Imputer from ``sklearn.preprocessing`` was used to solve this problem -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imp = Imputer(missing_values='NaN', strategy='most_frequent', axis=0)\n",
    "imp = imp.fit(X_train)\n",
    "X_train_imp = imp.transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Solution\n",
    "\n",
    "### The ML Solution\n",
    "After cleaning the data, we could proceed to use the attribute values to predict the Empathy of any person. The following classifiers were analyzed in this project -\n",
    "\n",
    "#### 3.1 Decision Tree Classifier\n",
    "We first started with a simple Decision Tree Classifier, which tries to predict the empathy score by using simple decision rules inferred from the training data that we provide it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.34702970297\n"
     ]
    }
   ],
   "source": [
    "from dt import *\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "resArray = np.zeros(10)\n",
    "for i in range(0, 10):\n",
    "    clf.fit(X=X_train_imp, y=y_train_imp)\n",
    "    clf.feature_importances_ # [ 1.,  0.,  0.]\n",
    "    result = clf.score(X=X_dev_imp, y=y_dev_imp)\n",
    "    resArray[i] = result\n",
    "\n",
    "print(str(np.mean(resArray)))\n",
    "#0.34702970297"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 3.2 Random Forest Classifier\n",
    "The Random Forest Classifier is an ensemble method, in which each tree in the ensemble is built from a bootstrap sample from the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.355445544554\n"
     ]
    }
   ],
   "source": [
    "from randomForest import *\n",
    "clf = RandomForestClassifier(n_estimators=10)\n",
    "resArray = np.zeros(10)\n",
    "for i in range(0, 10):\n",
    "    clf = clf.fit(X=X_train_imp, y=y_train_imp)\n",
    "    result = clf.score(X=X_dev_imp, y=y_dev_imp) \n",
    "    resArray[i] = result\n",
    "\n",
    "print(str(np.mean(resArray)))\n",
    "#0.355445544554"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Support Vector Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from svm import *\n",
    "clf = SVC()\n",
    "resArray = np.zeros(10)\n",
    "for i in range(0, 10):\n",
    "    clf = clf.fit(X=X_train_imp, y=y_train_imp)\n",
    "    result = clf.score(X=X_dev_imp, y=y_dev_imp) \n",
    "    resArray[i] = result\n",
    "\n",
    "print(str(np.mean(resArray)))\n",
    "#0.470297029703"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the SVM model achieved the highest accuracy, I further tried to perform bagging on the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bagging = BaggingClassifier(svm.SVC(), max_samples=0.5, max_features=0.5)\n",
    "resArray = np.zeros(10)\n",
    "for i in range(0, 10):\n",
    "    bagging = bagging.fit(X=X_train_imp, y=y_train_imp)\n",
    "    result = bagging.score(X=X_dev_imp, y=y_dev_imp) \n",
    "    resArray[i] = result\n",
    "\n",
    "print(str(np.mean(resArray)))\n",
    "#0.372277227723"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, this led to a drop in accuracy, as a result I removed bagging and began tuning the hyperparameter C using the development data. Using this value of C we predict on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C = \n",
      "1\n",
      "0.435643564356\n"
     ]
    }
   ],
   "source": [
    "#bagging = BaggingClassifier(svm.SVC(), max_samples=0.5, max_features=0.5)\n",
    "#0.372277227723\n",
    "resArray = np.zeros(10)\n",
    "for i in range(10):\n",
    "    clf = svm.SVC(C=i+1)\n",
    "    clf = clf.fit(X=X_train_imp, y=y_train_imp)\n",
    "    result = clf.score(X=X_dev_imp, y=y_dev_imp) \n",
    "    resArray[i] = result\n",
    "\n",
    "#tuning hyperparameter C\n",
    "print('C = ')\n",
    "print(str(np.argmax(resArray)))\n",
    "c = np.argmax(resArray) + 1\n",
    "\n",
    "clf = svm.SVC(C=c)\n",
    "for i in range(10):\n",
    "    clf = clf.fit(X=X_train_imp, y=y_train_imp)\n",
    "    result = clf.score(X=X_test_imp, y=y_test_imp) \n",
    "    resArray[i] = result\n",
    "    \n",
    "print(str(np.mean(resArray)))\n",
    "#0.435643564356"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4 AdaBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.376237623762\n"
     ]
    }
   ],
   "source": [
    "from adaBoost import *\n",
    "clf = AdaBoostClassifier(n_estimators=100)\n",
    "resArray = np.zeros(10)\n",
    "for i in range(0, 10):\n",
    "    clf = clf.fit(X=X_train_imp, y=y_train_imp)\n",
    "    result = clf.score(X=X_dev_imp, y=y_dev_imp) \n",
    "    resArray[i] = result\n",
    "\n",
    "print(str(np.mean(resArray)))\n",
    "#0.376237623762"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5 Multi-layer Perceptron Classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.386138613861\n"
     ]
    }
   ],
   "source": [
    "from mlp import *\n",
    "clf = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=1)\n",
    "resArray = np.zeros(10)\n",
    "for i in range(0, 10):\n",
    "    clf = clf.fit(X=X_train_imp, y=y_train_imp)\n",
    "    result = clf.score(X=X_dev_imp, y=y_dev_imp) \n",
    "    resArray[i] = result\n",
    "\n",
    "print(str(np.mean(resArray)))\n",
    "#0.386138613861"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.6 Voting Classifier\n",
    "In case of Voting Classifier, multiple classifiers are considered and are each given votes/ weightage depending on which a prediction is made. Thus, I tried combining all the above classifiers & tried various combinations to achieve the best accuracy possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.438613861386\n"
     ]
    }
   ],
   "source": [
    "from votingClassifier import *\n",
    "clf1 = tree.DecisionTreeClassifier()\n",
    "clf2 = RandomForestClassifier(n_estimators=10)\n",
    "clf3 = svm.SVC()\n",
    "clf4 = AdaBoostClassifier(n_estimators=100)\n",
    "clf5 = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=1)\n",
    "\n",
    "eclf = VotingClassifier(estimators=[('dt', clf1), ('rf', clf2), ('svm', clf3), ('ada', clf4), ('mlp', clf5)], voting='hard')\n",
    "resArray = np.zeros(10)\n",
    "for i in range(0, 10):\n",
    "    eclf = eclf.fit(X=X_train_imp, y=y_train_imp)\n",
    "    result = eclf.score(X=X_dev_imp, y=y_dev_imp) # 1.0\n",
    "    resArray[i] = result\n",
    "\n",
    "print(str(np.mean(resArray)))\n",
    "#0.373762376238"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The corresponding combinations and their respective accuracies are listed below -\n",
    "\n",
    "``[('dt', clf1), ('rf', clf2), ('svm', clf3), ('ada', clf4)], voting='hard')\n",
    "\n",
    "0.383663366337\n",
    "\n",
    "[('dt', clf1), ('rf', clf2), ('svm', clf3), ('ada', clf4), ('mlp', clf5)]\n",
    "\n",
    "0.373762376238\n",
    "\n",
    "[('rf', clf2), ('svm', clf3), ('ada', clf4), ('mlp', clf5)]\n",
    "\n",
    "0.390594059406\n",
    "\n",
    "[('rf', clf2), ('svm', clf3), ('ada', clf4)]\n",
    "\n",
    "0.384653465347\n",
    "\n",
    "[('rf', clf2), ('svm', clf3)]\n",
    "\n",
    "0.350495049505\n",
    "\n",
    "[('rf', clf2), ('svm', clf3), ('mlp', clf5)]\n",
    "\n",
    "0.433663366337``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Evaluation Process\n",
    "For the classification process, the data was split into three categories (training, development and testing) in the ratio 60:20:20. Accuracy was used as a parameter to evaluate classifiers.\n",
    "* All the classification models were trained using the training data\n",
    "* Once we found the model with the best accuracy we try to tune the hyperparameters using the development data\n",
    "* Lastly, we calculate the accuracy for the test data.\n",
    "\n",
    "### The Result\n",
    "SVM performed the best amongst all the classifiers, with an average accuracy of 47.03%. On performing bagging on SVM accuracy dropped to 37.22%, so it wasn’t considered in the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
